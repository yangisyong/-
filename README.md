
  本项目旨在通过计算机视觉和深度学习技术实现中国手语动作的实时识别，以帮助听障人士与外界进
行更便捷的交流。系统采用 MediaPipe 提取人体关键点（包括手部、面部和姿势），并构建了一个基于 LSTM
的深度学习模型，用于对关键点序列进行分类。数据集包含六个常见手语动作（如“你好”、“谢谢”、“对不
起”、“玩”、“可以”、“帮助”），每个动作采集了 65 个序列，每个序列包含 40 帧的关键点数据。
模型在训练集上进行了 150 轮训练，最终在测试集上达到了 90% 以上的准确率。实时检测部分通过摄像
头捕获视频流，动态提取关键点并预测动作类别，同时使用中文文本和概率条可视化预测结果。实验表明，
该系统具有较高的准确性和实时性，能够有效识别特定手语动作。未来将进一步扩展动作种类并优化模型
性能，以提升系统的实用性。
#系统分为数据采集、特征提取、模型训练和实时检测四个模块。
#-依赖库：tensorflow  opencv-python  mediapipe  scikit-learn matplotlib

数据集：手语数据集，数据类型为numpy的数组形式数据（是由mediapipe收集的手部和脸部以及姿态特征如x,y,z,visibility） 数组长度为1662.

手语识别.ipython 是整个过程的代码。

使用了 LSTM，但我使用的激活函数是 tanh ，我使用了 Dropout Layer 和 l2 正则化来防止过拟合

测试.py是结果程序

训练过程：就在手语识别.ipython中
